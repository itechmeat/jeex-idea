"""
Comprehensive performance benchmarks for vector database system.

Validates performance requirements from Setup Vector Database story:
- REQ-007: Query Performance Optimization - P95 latency < 100ms
- PERF-001: Search Performance - P95 < 100ms at 100K vectors
- PERF-002: Indexing Performance - Batch upsert â‰¥ 200 vectors/second
- SCALE-002: Concurrent Query Capacity - â‰¥ 100 QPS
"""

import asyncio
import gc
import json
import random
import time
import statistics
from datetime import datetime
from typing import Dict, List, Any, Tuple, Optional
from uuid import uuid4, UUID
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil
import pytest
from httpx import AsyncClient

from app.services.vector.domain.entities import (
    VectorPoint,
    VectorData,
    SearchContext,
    DocumentType,
    ProjectId,
    LanguageCode,
)
from app.services.vector.search_service import DefaultVectorSearchService
from app.services.vector.repositories.qdrant_repository import QdrantVectorRepository
from app.services.vector.collection_manager import CollectionManager
from qdrant_client import QdrantClient


@dataclass
class PerformanceMetrics:
    """Data class for performance measurement results."""

    operation: str
    dataset_size: int
    duration_ms: float
    throughput: float  # operations per second
    latency_p50: float
    latency_p95: float
    latency_p99: float
    error_count: int
    memory_usage_mb: float
    cpu_usage_percent: float
    additional_data: Dict[str, Any] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        data = asdict(self)
        if self.additional_data:
            data.update(self.additional_data)
        return data


@dataclass
class BenchmarkConfig:
    """Configuration for performance benchmarks."""

    # Dataset sizes for scaling tests
    dataset_sizes = [1000, 10000, 50000, 100000]

    # Batch sizes for upsert testing
    batch_sizes = [10, 50, 100]

    # Concurrency levels
    concurrency_levels = [1, 10, 50, 100]

    # Performance targets from requirements
    target_p95_latency_ms = 100.0  # REQ-007, PERF-001
    target_upsert_throughput = 200.0  # PERF-002 vectors/second
    target_qps = 100.0  # SCALE-002

    # Test configuration
    warmup_iterations = 3
    benchmark_iterations = 10
    timeout_seconds = 30

    # Vector dimensions (matching OpenAI embeddings)
    vector_dimension = 1536


class VectorDataGenerator:
    """Generates realistic test data for performance testing."""

    def __init__(self, dimension: int = 1536):
        self.dimension = dimension
        self.content_templates = [
            "Machine learning algorithms transform data into insights through statistical patterns and computational methods.",
            "Vector databases enable efficient similarity search using high-dimensional embeddings and approximate nearest neighbors.",
            "Project isolation ensures data security by enforcing strict boundaries between different user contexts.",
            "Natural language processing models understand semantic relationships through contextual embeddings.",
            "Distributed systems maintain consistency and availability through coordination protocols and fault tolerance.",
            "Cloud computing provides scalable infrastructure with elastic resource allocation and load balancing.",
            "Artificial intelligence systems learn from training data to make predictions and decisions.",
            "Software architecture patterns guide the design of maintainable and scalable applications.",
            "Database optimization techniques improve query performance through indexing and caching strategies.",
            "Security frameworks protect sensitive data through encryption, authentication, and access control.",
        ]

    def generate_vector(self) -> List[float]:
        """Generate realistic normalized embedding vector."""
        # Generate random vector and normalize
        vector = [random.uniform(-1, 1) for _ in range(self.dimension)]

        # Normalize to unit vector (realistic for embeddings)
        magnitude = sum(x * x for x in vector) ** 0.5
        if magnitude > 0:
            vector = [x / magnitude for x in vector]

        return vector

    def generate_vector_point(
        self,
        project_id: str,
        language: str = "en",
        document_type: DocumentType = DocumentType.KNOWLEDGE,
    ) -> VectorPoint:
        """Generate a realistic vector point with metadata."""
        content = random.choice(self.content_templates)
        vector = self.generate_vector()

        return VectorPoint(
            vector=VectorData(vector),
            content=content,
            project_id=ProjectId(project_id),
            language=LanguageCode(language),
            document_type=document_type,
            title=f"Test Document {uuid4().hex[:8]}",
            importance=random.uniform(0.3, 1.0),
            metadata={
                "source": "performance_test",
                "category": random.choice(["technical", "business", "research"]),
                "tags": [
                    f"tag_{i}" for i in random.sample(range(100), random.randint(1, 5))
                ],
            },
        )

    def generate_query_vector(self) -> VectorData:
        """Generate a query vector for search testing."""
        return VectorData(self.generate_vector())


class PerformanceMonitor:
    """Monitor system resources during performance tests."""

    def __init__(self):
        self.process = psutil.Process()
        self.measurements = []

    def start_monitoring(self):
        """Start resource monitoring."""
        self.measurements = []

    def take_measurement(self) -> Dict[str, float]:
        """Take a single resource measurement."""
        return {
            "memory_mb": self.process.memory_info().rss / 1024 / 1024,
            "cpu_percent": self.process.cpu_percent(),
            "timestamp": time.time(),
        }

    def stop_monitoring(self) -> Dict[str, float]:
        """Stop monitoring and return aggregate metrics."""
        if not self.measurements:
            return {"memory_mb": 0.0, "cpu_percent": 0.0}

        return {
            "memory_mb": statistics.mean(m["memory_mb"] for m in self.measurements),
            "cpu_percent": statistics.mean(m["cpu_percent"] for m in self.measurements),
        }


@pytest.fixture
async def performance_setup():
    """Setup performance testing environment with isolated test collection."""
    # Create isolated test collection for performance tests
    test_collection_name = "jeex_performance_test"

    # Initialize Qdrant client
    client = QdrantClient(host="localhost", port=5230)
    repository = QdrantVectorRepository(client)

    # Override collection name for performance tests
    original_collection_name = repository.COLLECTION_NAME
    repository.COLLECTION_NAME = test_collection_name

    # Setup collection
    await repository.initialize_collection()

    # Create search service
    search_service = DefaultVectorSearchService(repository)

    yield repository, search_service, test_collection_name

    # Cleanup test collection
    try:
        await asyncio.to_thread(
            client.delete_collection, collection_name=test_collection_name
        )
    except Exception:
        pass  # Ignore cleanup errors


class VectorPerformanceBenchmark:
    """Main performance benchmarking class."""

    def __init__(
        self,
        repository: QdrantVectorRepository,
        search_service: DefaultVectorSearchService,
    ):
        self.repository = repository
        self.search_service = search_service
        self.data_generator = VectorDataGenerator()
        self.monitor = PerformanceMonitor()
        self.config = BenchmarkConfig()

    async def benchmark_search_performance(
        self, dataset_size: int, project_id: str, iterations: int = 10
    ) -> PerformanceMetrics:
        """
        Benchmark search performance across different dataset sizes.

        Validates REQ-007 and PERF-001: P95 latency < 100ms
        """
        print(f"ðŸ” Benchmarking search performance with {dataset_size} vectors...")

        # Setup test data
        project_id = str(uuid4())
        context = SearchContext.create(project_id, "en")

        # Generate and insert test data
        test_points = [
            self.data_generator.generate_vector_point(project_id)
            for _ in range(dataset_size)
        ]

        await self.repository.upsert_points(UUID(project_id), test_points)

        # Generate query vectors
        query_vectors = [
            self.data_generator.generate_query_vector() for _ in range(iterations)
        ]

        # Warmup runs
        for _ in range(self.config.warmup_iterations):
            query_vector = random.choice(query_vectors)
            await self.search_service.search(query_vector, context, limit=10)

        # Benchmark runs
        self.monitor.start_monitoring()
        latencies = []
        errors = 0
        start_time = time.time()

        for query_vector in query_vectors:
            try:
                query_start = time.time()
                await self.search_service.search(query_vector, context, limit=10)
                query_end = time.time()

                latencies.append((query_end - query_start) * 1000)  # Convert to ms

                # Take resource measurement
                measurement = self.monitor.take_measurement()
                self.monitor.measurements.append(measurement)

            except Exception as e:
                errors += 1
                print(f"Search error: {e}")

        total_time = (time.time() - start_time) * 1000  # Convert to ms
        resource_metrics = self.monitor.stop_monitoring()

        # Calculate statistics
        successful_queries = len(latencies)
        throughput = successful_queries / (total_time / 1000) if total_time > 0 else 0

        return PerformanceMetrics(
            operation="search",
            dataset_size=dataset_size,
            duration_ms=total_time,
            throughput=throughput,
            latency_p50=statistics.median(latencies) if latencies else 0,
            latency_p95=self._percentile(latencies, 95) if latencies else 0,
            latency_p99=self._percentile(latencies, 99) if latencies else 0,
            error_count=errors,
            memory_usage_mb=resource_metrics["memory_mb"],
            cpu_usage_percent=resource_metrics["cpu_percent"],
            additional_data={
                "successful_queries": successful_queries,
                "target_p95_ms": self.config.target_p95_latency_ms,
                "meets_target": self._percentile(latencies, 95)
                <= self.config.target_p95_latency_ms
                if latencies
                else False,
            },
        )

    async def benchmark_upsert_performance(
        self,
        batch_size: int,
        total_vectors: int = 1000,
        project_id: Optional[str] = None,
    ) -> PerformanceMetrics:
        """
        Benchmark batch upsert performance.

        Validates PERF-002: â‰¥ 200 vectors/second throughput
        """
        print(f"ðŸ“ Benchmarking upsert performance with batch size {batch_size}...")

        if project_id is None:
            project_id = str(uuid4())

        # Generate test data
        test_points = [
            self.data_generator.generate_vector_point(project_id)
            for _ in range(total_vectors)
        ]

        # Warmup run
        warmup_batch = test_points[: min(batch_size, 10)]
        await self.repository.upsert_points(UUID(project_id), warmup_batch)

        # Benchmark runs
        self.monitor.start_monitoring()
        latencies = []
        errors = 0
        start_time = time.time()

        # Process in batches
        for i in range(0, len(test_points), batch_size):
            batch = test_points[i : i + batch_size]

            try:
                batch_start = time.time()
                await self.repository.upsert_points(UUID(project_id), batch)
                batch_end = time.time()

                latencies.append((batch_end - batch_start) * 1000)  # Convert to ms

                # Take resource measurement
                measurement = self.monitor.take_measurement()
                self.monitor.measurements.append(measurement)

            except Exception as e:
                errors += 1
                print(f"Upsert error: {e}")

        total_time = time.time() - start_time
        resource_metrics = self.monitor.stop_monitoring()

        # Calculate statistics
        successful_vectors = total_vectors - (errors * batch_size)
        throughput = successful_vectors / total_time if total_time > 0 else 0

        return PerformanceMetrics(
            operation="upsert",
            dataset_size=total_vectors,
            duration_ms=total_time * 1000,
            throughput=throughput,
            latency_p50=statistics.median(latencies) if latencies else 0,
            latency_p95=self._percentile(latencies, 95) if latencies else 0,
            latency_p99=self._percentile(latencies, 99) if latencies else 0,
            error_count=errors,
            memory_usage_mb=resource_metrics["memory_mb"],
            cpu_usage_percent=resource_metrics["cpu_percent"],
            additional_data={
                "batch_size": batch_size,
                "successful_vectors": successful_vectors,
                "target_throughput": self.config.target_upsert_throughput,
                "meets_target": throughput >= self.config.target_upsert_throughput,
            },
        )

    async def benchmark_concurrent_search(
        self, concurrency: int, total_queries: int = 100, dataset_size: int = 10000
    ) -> PerformanceMetrics:
        """
        Benchmark concurrent search performance.

        Validates SCALE-002: â‰¥ 100 QPS capacity
        """
        print(
            f"âš¡ Benchmarking concurrent search with {concurrency} concurrent queries..."
        )

        # Setup test data
        project_id = str(uuid4())
        context = SearchContext.create(project_id, "en")

        # Generate test dataset
        test_points = [
            self.data_generator.generate_vector_point(project_id)
            for _ in range(dataset_size)
        ]
        await self.repository.upsert_points(UUID(project_id), test_points)

        # Generate query vectors
        query_vectors = [
            self.data_generator.generate_query_vector() for _ in range(total_queries)
        ]

        # Warmup
        for _ in range(min(5, total_queries)):
            query_vector = random.choice(query_vectors)
            await self.search_service.search(query_vector, context, limit=10)

        # Concurrent benchmark
        self.monitor.start_monitoring()
        latencies = []
        errors = 0
        start_time = time.time()

        async def execute_search(query_vector: VectorData) -> Tuple[float, bool]:
            """Execute single search and return (latency, success)."""
            try:
                query_start = time.time()
                await self.search_service.search(query_vector, context, limit=10)
                query_end = time.time()
                return (query_end - query_start) * 1000, True
            except Exception as e:
                print(f"Concurrent search error: {e}")
                return (0, False)

        # Execute queries concurrently
        semaphore = asyncio.Semaphore(concurrency)

        async def bounded_search(query_vector: VectorData) -> Tuple[float, bool]:
            async with semaphore:
                return await execute_search(query_vector)

        # Run all queries
        tasks = [bounded_search(qv) for qv in query_vectors]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results
        successful_latencies = []
        for result in results:
            if isinstance(result, Exception):
                errors += 1
            else:
                latency, success = result
                if success:
                    successful_latencies.append(latency)
                else:
                    errors += 1

        total_time = time.time() - start_time
        resource_metrics = self.monitor.stop_monitoring()

        # Calculate statistics
        successful_queries = len(successful_latencies)
        qps = successful_queries / total_time if total_time > 0 else 0

        return PerformanceMetrics(
            operation="concurrent_search",
            dataset_size=dataset_size,
            duration_ms=total_time * 1000,
            throughput=qps,
            latency_p50=statistics.median(successful_latencies)
            if successful_latencies
            else 0,
            latency_p95=self._percentile(successful_latencies, 95)
            if successful_latencies
            else 0,
            latency_p99=self._percentile(successful_latencies, 99)
            if successful_latencies
            else 0,
            error_count=errors,
            memory_usage_mb=resource_metrics["memory_mb"],
            cpu_usage_percent=resource_metrics["cpu_percent"],
            additional_data={
                "concurrency": concurrency,
                "total_queries": total_queries,
                "successful_queries": successful_queries,
                "target_qps": self.config.target_qps,
                "meets_target": qps >= self.config.target_qps,
            },
        )

    async def benchmark_filter_performance(self) -> PerformanceMetrics:
        """Benchmark search performance with different filter combinations."""
        print("ðŸ” Benchmarking filter performance...")

        # Setup test data with multiple projects and languages
        project_ids = [str(uuid4()) for _ in range(5)]
        languages = ["en", "es", "fr", "de", "it"]

        # Group points by project for secure upsert
        points_by_project = {}
        for project_id in project_ids:
            points_by_project[project_id] = []
            for language in languages:
                for doc_type in DocumentType:
                    # Generate points for each combination
                    for _ in range(100):
                        point = self.data_generator.generate_vector_point(
                            project_id=project_id,
                            language=language,
                            document_type=doc_type,
                        )
                        points_by_project[project_id].append(point)

        # Upsert points per project with mandatory project_id validation
        for project_id, test_points in points_by_project.items():
            await self.repository.upsert_points(UUID(project_id), test_points)

        # Test different filter combinations
        filter_tests = [
            {"project_id": project_ids[0], "language": "en"},
            {
                "project_id": project_ids[1],
                "language": "es",
                "document_type": DocumentType.KNOWLEDGE,
            },
            {
                "project_id": project_ids[2],
                "language": "fr",
                "document_type": DocumentType.MEMORY,
                "importance_min": 0.7,
            },
        ]

        latencies = []
        errors = 0
        start_time = time.time()

        for filter_config in filter_tests:
            project_id = filter_config["project_id"]
            language = filter_config["language"]
            context = SearchContext.create(project_id, language)
            query_vector = self.data_generator.generate_query_vector()

            # Run multiple queries per filter configuration
            for _ in range(10):
                try:
                    query_start = time.time()

                    # Build filters
                    filters = {}
                    if "document_type" in filter_config:
                        filters["document_type"] = filter_config["document_type"].value
                    if "importance_min" in filter_config:
                        filters["importance_min"] = filter_config["importance_min"]

                    await self.search_service.search(
                        query_vector, context, limit=10, filters=filters
                    )

                    query_end = time.time()
                    latencies.append((query_end - query_start) * 1000)

                except Exception as e:
                    errors += 1
                    print(f"Filter search error: {e}")

        total_time = (time.time() - start_time) * 1000

        return PerformanceMetrics(
            operation="filter_search",
            dataset_size=len(test_points),
            duration_ms=total_time,
            throughput=len(latencies) / (total_time / 1000) if total_time > 0 else 0,
            latency_p50=statistics.median(latencies) if latencies else 0,
            latency_p95=self._percentile(latencies, 95) if latencies else 0,
            latency_p99=self._percentile(latencies, 99) if latencies else 0,
            error_count=errors,
            memory_usage_mb=0,  # Not tracked for this test
            cpu_usage_percent=0,
            additional_data={
                "filter_combinations": len(filter_tests),
                "queries_per_filter": 10,
                "total_filter_queries": len(latencies),
            },
        )

    def _percentile(self, data: List[float], percentile: float) -> float:
        """Calculate percentile of data list."""
        if not data:
            return 0.0
        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]


# pytest benchmark tests
@pytest.mark.asyncio
@pytest.mark.performance
class TestVectorPerformance:
    """Performance test suite for vector database operations."""

    async def test_search_performance_scaling(self, performance_setup):
        """Test search performance across different dataset sizes."""
        repository, search_service, collection_name = performance_setup
        benchmark = VectorPerformanceBenchmark(repository, search_service)

        results = []
        for dataset_size in benchmark.config.dataset_sizes:
            if dataset_size > 50000:  # Skip very large datasets for quick testing
                continue

            metrics = await benchmark.benchmark_search_performance(
                dataset_size, str(uuid4())
            )
            results.append(metrics.to_dict())

            # Validate performance target
            assert metrics.latency_p95 <= benchmark.config.target_p95_latency_ms, (
                f"P95 latency {metrics.latency_p95:.2f}ms exceeds target {benchmark.config.target_p95_latency_ms}ms"
            )

        print(
            f"âœ… Search performance scaling test completed: {len(results)} dataset sizes tested"
        )

    async def test_upsert_performance_batch_sizes(self, performance_setup):
        """Test upsert performance with different batch sizes."""
        repository, search_service, collection_name = performance_setup
        benchmark = VectorPerformanceBenchmark(repository, search_service)

        results = []
        for batch_size in benchmark.config.batch_sizes:
            metrics = await benchmark.benchmark_upsert_performance(
                batch_size=batch_size, total_vectors=1000
            )
            results.append(metrics.to_dict())

            # Validate throughput target
            assert metrics.throughput >= benchmark.config.target_upsert_throughput, (
                f"Upsert throughput {metrics.throughput:.2f} vectors/s below target {benchmark.config.target_upsert_throughput}"
            )

        print(
            f"âœ… Upsert performance test completed: {len(results)} batch sizes tested"
        )

    async def test_concurrent_search_capacity(self, performance_setup):
        """Test concurrent search performance."""
        repository, search_service, collection_name = performance_setup
        benchmark = VectorPerformanceBenchmark(repository, search_service)

        results = []
        for concurrency in [10, 50]:  # Test moderate concurrency for faster tests
            metrics = await benchmark.benchmark_concurrent_search(
                concurrency=concurrency, total_queries=50
            )
            results.append(metrics.to_dict())

        print(
            f"âœ… Concurrent search test completed: {len(results)} concurrency levels tested"
        )

    async def test_filter_performance(self, performance_setup):
        """Test search performance with various filters."""
        repository, search_service, collection_name = performance_setup
        benchmark = VectorPerformanceBenchmark(repository, search_service)

        metrics = await benchmark.benchmark_filter_performance()

        # Filter searches should still be reasonably fast
        assert metrics.latency_p95 <= 200, (
            f"Filter search P95 latency {metrics.latency_p95:.2f}ms exceeds 200ms threshold"
        )

        print(
            f"âœ… Filter performance test completed: {metrics.latency_p95:.2f}ms P95 latency"
        )


async def test_vector_performance_main():
    """Main entry point for running performance tests directly."""
    print("ðŸš€ Running Vector Database Performance Tests")

    # Run the performance test suite
    pytest.main(
        [__file__, "-v", "--tb=short", "-m", "performance", "--asyncio-mode=auto"]
    )


if __name__ == "__main__":
    # Run performance tests directly
    asyncio.run(test_vector_performance_main())
